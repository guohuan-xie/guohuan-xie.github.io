---
permalink: /
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---
<html> 
<head>
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Permanent+Marker&display=swap');
        @import url('https://fonts.googleapis.com/css2?family=Fredericka+the+Great&display=swap');
        @import url('https://fonts.googleapis.com/css2?family=Homemade+Apple&display=swap');
        body {
            background-color:	 #FFFFFF;
            font-family: 'Arial Rounded MT Bold', 'Verdana', sans-serif;
            font-size: 15px;
        }
        .main-heading {
            font-family: 'Permanent Marker', cursive;
            text-align: center;
            color: #ca6f6f;
        }
        div.markdown-body a, a {
           text-decoration: none !important;
           color: #ca6f6f;
        }
    </style>
</head>
<body>
<h1 class="main-heading">Hi there <img src="images/Hi.gif" width="40px"> Welcome to my academic base!</h1>
</body>
</html>
I am a third-year undergraduate(2022-2026) at [Nankai University](https://www.nankai.edu.cn/), currently pursuing research in Computer Vision under the guidance of [Prof. Yun Liu](https://yun-liu.github.io/) at the [Media Computing Lab](https://mmcheng.net/), [Nankai University](https://www.nankai.edu.cn/). My current work centers on designing deep learning models that effectively capture spatiotemporal consistency in videos and integrate information across different modalities (e.g., visual and textual cues) to enhance scene understanding. I am particularly interested in advancing video perception systems through efficient architectures, label-efficient learning, and cross-modal alignment techniques.
<span style="color: #83a1c7;">I am looking for 2026 fall phd or master opportunities, alongside potential internships, whether in academia or industry</span>.

Research Interests
--------
My research interests and the learning paradigm I aim to shape primarily focus on:

- **Multimodal and Embodied Intelligence**: Exploring how agents can understand and interact with the world by integrating visual, linguistic, and embodied signals. I am particularly interested in enabling perception-action loops that connect vision, language, and physical environments for real-world decision making.

- **Efficient Visual Learning**: Investigating model acceleration, lightweight architectures, and training strategies for high-performance visual understanding, with the goal of making computer vision models more scalable, deployable, and efficient in real-world applications.

News
---------------
- *I have set up a [Blog Site](https://guohuan-xie.github.io), welcome everyone to visit!*

Research Experience
--------------
<div style="display: flex; align-items: center;">
    <img src="images/nku.png" alt="Nankai logo" width="60" height="60" style="margin-right: 20px;">
    <div>
        <strong>Nankai University (NKU)</strong><br>
        August 2024 -  now<br>
        Research intern at <a href="https://mmcheng.net/"><em>Media Computing Lab</em></a> Lab 
    </div>
</div>

Publications
--------------
 <div style="display: flex; align-items: center;">
    <img src="images/dsp.png" alt="DSP" width="200" height="100" style="margin-right: 20px;">
    <div>
        <strong>Dense Policy: Bidirectional Autoregressive Learning of Actions</strong><br>
        <i style="font-size: 13px;">
            <a href="https://selen-suyue.github.io" target="_blank"><strong>Yue Su</strong></a>*, 
            <a href="https://scholar.google.com/citations?user=WurpqEMAAAAJ&hl=en" target="_blank">Xinyu Zhan</a>*, 
            <a href="https://tonyfang.net/" target="_blank">Hongjie Fang</a>, 
            <a href="https://hanxue.me/" target="_blank">Han Xue</a>, <br>
            <a href="https://fang-haoshu.github.io/" target="_blank">Haoshu Fang</a>, 
            <a href="https://dirtyharrylyl.github.io/" target="_blank">Yong-Lu Li</a>, 
            <a href="http://mvig.org" target="_blank">Cewu Lu</a>, 
            <a href="https://lixiny.github.io" target="_blank">Lixin Yang</a>&dagger;
        </i><br>
        Propose Dense Policy, A bidirectional robotic autoregressive policy, which infers trajectories by gradually expanding actions from sparse keyframes, has demonstrated capabilities exceeding diffusion policies.<br>
        <a href="https://arxiv.org/abs/2503.13217"><em>[arXiv]</em></a>
        <a href="https://selen-suyue.github.io/DspNet/"><em>[website]</em></a>
        <a href="https://github.com/Selen-Suyue/DensePolicy"><em>[3D-code]</em></a>
      <a href="https://github.com/Selen-Suyue/DensePolicy2D"><em>[2D-code]</em></a>
    </div>
</div>
<br>

Projects
--------
<div style="display: flex; align-items: center;">
    <img src="images/nkuassistant.jpg" alt="NKU-AI-Assistant" width="200" height="100" style="margin-right: 20px;">
    <div>
        <strong>NKU-AI-Assistant: Multi-Agent Conversational Module with RAG Integration</strong><br> 
        The NKU-AI-Assistant is a conversational AI module designed to handle natural language interactions and complex user queries through a combination of multi-agent architecture and Retrieval-Augmented Generation (RAG) techniques. It features three specialized agents—ChainMind (a lightweight LangChain agent for casual dialogue), MetaAgent (a LangGraph-based multi-agent system for complex, multi-step reasoning), and NeoGraph (a GraphRAG agent that utilizes knowledge graphs and graph databases for structured information retrieval). Together, these agents enable dynamic task decomposition, precise information access, and robust response generation. Complementing the agents are four functional plugins—for parsing web pages, YouTube videos, uploaded documents, and translating academic literature—further enhancing the assistant’s versatility and intelligence in real-world applications.
<br>
      <a href="https://github.com/guohuan-xie/NKU-AI-Assistant"><em>[code]</em></a> 
      <a href="https://guohuan-xie.github.io/files/nku.mp4"><em>[demo]</em></a>
    </div>
</div>

Awards
---------------
- *National Scholarship 2024, National Level. 0.4%*
- *Second Prize, National Level, 2024 China Mathematical Contest in Modeling.*
- *Seond Prize, Provincial Level, 2023 Tianjin Mathematics Competition.*





