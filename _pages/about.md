---
permalink: /
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---
<html> 
<head>
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Permanent+Marker&display=swap');
        @import url('https://fonts.googleapis.com/css2?family=Fredericka+the+Great&display=swap');
        @import url('https://fonts.googleapis.com/css2?family=Homemade+Apple&display=swap');
        body {
            background-color:	 #FFFFFF;
            font-family: 'Arial Rounded MT Bold', 'Verdana', sans-serif;
            font-size: 15px;
        }
        .main-heading {
            font-family: 'Permanent Marker', cursive;
            text-align: center;
            color: #ca6f6f;
        }
        div.markdown-body a, a {
           text-decoration: none !important;
           color: #ca6f6f;
        }
    </style>
</head>
<body>
<h1 class="main-heading">Hi there <img src="images/Hi.gif" width="40px"> Welcome to my academic base!</h1>
</body>
</html>
I am a third-year undergraduate(2022-2026) at [Nankai University](https://www.nankai.edu.cn/), currently pursuing research in Computer Vision under the guidance of [Prof. Yun Liu](https://yun-liu.github.io/) at the [Media Computing Lab](https://mmcheng.net/), [Nankai University](https://www.nankai.edu.cn/). My research explores multi-modal perception and generative vision models. I focus on integrating visual-language information for deeper scene understanding and reasoning, and leveraging models like diffusion and VAR for segmentation and editing, with emphasis on detail restoration, cross-modal generalization, and efficient training for practical visual perception tasks.
<span style="color: #83a1c7;">I am looking for 2026 fall phd or master opportunities, alongside potential internships, whether in academia or industry</span>.

Research Interests
--------
My research interests and the learning paradigm I aim to shape primarily focus on:

- **Multimodal Understanding**: Investigating how to integrate visual and linguistic modalities to construct a perceptual feedback loop, enabling deeper scene understanding and reasoning. I am particularly interested in supporting complex perception and decision-making through cross-modal alignment in real-world tasks.

- **Generative Model-driven Visual Understanding**: Exploring the use of generative models (e.g., diffusion, VAR) for semantic segmentation and image editing, with a focus on fine-detail reconstruction, cross-modal generalization, and efficient training, aiming to enhance their applicability and robustness in visual perception systems.


News
---------------
- *I have set up a [Blog Site](https://guohuan-xie.github.io), welcome everyone to visit!*

Research Experience
--------------
<div style="display: flex; align-items: center;">
    <img src="images/nku.png" alt="Nankai logo" width="60" height="60" style="margin-right: 20px;">
    <div>
        <strong>Nankai University (NKU)</strong><br>
        August 2024 -  now<br>
        Research intern at <a href="https://mmcheng.net/"><em>Media Computing Lab</em></a>
    </div>
</div>

Publications
--------------
 <div style="display: flex; align-items: center;">
    <img src="images/vpssurvey.png" alt="DSP" width="200" height="100" style="margin-right: 20px;">
    <div>
        <strong>A Comprehensive Survey on Video Scene Parsing: Advances, Challenges, and Prospects</strong><br>
        <i style="font-size: 13px;">
            <a href="https://guohuan-xie.github.io/" target="_blank"><strong>Guohuan Xie</strong></a>, 
            <a href="https://scholar.google.com/citations?user=Du1SYd0AAAAJ&hl=en" target="_blank">Syed Ariff Syed Hesham</a>, 
            <a href="https://scholar.google.com.tw/citations?user=XpqqcAYAAAAJ&hl=zh-CN" target="_blank">Wenya Guo</a>, 
            <a href="https://scholar.google.com.sg/citations?user=u7Gb-qwAAAAJ&hl=en" target="_blank">Bing Li</a>, <br>
            <a href="https://scholar.google.com/citations?user=huWpVyEAAAAJ&hl=en" target="_blank">Ming-Ming Cheng</a>, 
            <a href="https://scholar.google.com.hk/citations?user=qd8Blw0AAAAJ&hl=zh-CN" target="_blank">Guolei Sun</a>, 
            <a href="https://yun-liu.github.io/" target="_blank">Yun Liu</a>&dagger;
        </i><br>
        This survey comprehensively reviews Video Scene Parsing (VSP), encompassing VSS, VIS, VPS, VTS, and OVVS. It analyzes architectural advances from CNNs to Transformers, addresses core challenges like temporal consistency, and provides unified benchmarks, datasets, and future research perspectives.<br>
        <a href="https://arxiv.org/abs/2503.13217"><em>[arXiv]</em></a>
    </div>
</div>
<br>

Projects
--------
<div style="display: flex; align-items: center;">
    <img src="images/nkuassistant.jpg" alt="NKU-AI-Assistant" width="200" height="100" style="margin-right: 20px;">
    <div>
        <strong>NKU-AI-Assistant: Multi-Agent Conversational System with RAG</strong><br> 
        NKU-AI-Assistant is a conversational AI system using multi-agent architecture and RAG. It features ChainMind, MetaAgent, and NeoGraph for dialogue, reasoning, and knowledge retrieval, respectively, supported by web, video, file parsing, and translation plugins for enhanced versatility.
<br>
      <a href="https://github.com/guohuan-xie/NKU-AI-Assistant"><em>[code]</em></a> 
      <a href="https://guohuan-xie.github.io/files/nku.mp4"><em>[demo]</em></a>
    </div>
</div>

Awards
---------------
- *Meritorious Winner, International Level, 2025 Interdisciplinary Contest in Modeling (ICM).*
- *National Scholarship 2024, 0.4%*
- *Second Prize, National Level, 2024 China Mathematical Contest in Modeling.*
- *Seond Prize, Provincial Level, 2023 Tianjin Mathematics Competition.*
- *Gong-Neng Scholarship of Nankai University 2023, 5%*





